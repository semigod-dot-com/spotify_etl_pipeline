Spotify ETL & Analytics Automation with dbt and GitHub Actions
This project demonstrates a complete modern data pipeline built with Python, PostgreSQL, dbt, and GitHub Actions to automate the extraction, transformation, and loading (ETL) of Spotify listening data.

📌 Project Overview
Goal:
Track and analyze personal Spotify listening behavior using an automated and reproducible analytics pipeline.

⚙️ Tools & Stack
Python: For extraction and ingestion of Spotify data

Spotipy: For authenticated access to Spotify Web API

PostgreSQL: Local relational data warehouse

dbt (Data Build Tool): For data transformation and modeling

GitHub Actions: For CI/CD-style automation of the pipeline

papermill: Optional notebook execution (if used in automation)

Git + GitHub: For version control and deployment

Project Structure
semigod_socials/
├── extraction_scripts/         # Python & Notebook scripts
│   ├── spotify_extraction.py
│   ├── spotify_ingestion.py
│   ├── recent_tracks.csv
│
├── semi/                       # dbt project directory
│   ├── models/
│   ├── snapshots/
│   ├── dbt_project.yml
│
├── .github/
│   └── workflows/
│       └── spotify_pipeline.yml  # GitHub Actions for daily automation
│
├── requirements.txt            # All dependencies
└── summary                     # Documentation notes
🔁 Pipeline Workflow
Extraction:

Uses the Spotify Web API via Spotipy

Authenticates via OAuth and pulls recent listening history

Saves data as CSV for transparency & backup

Ingestion:

Loads extracted CSV into PostgreSQL using psycopg2 or SQLAlchemy

Transformation with dbt:

Stages and cleans raw data

Creates models: stg_recent_tracks, recent_tracks, incre_recent_tracks

Uses snapshots for slowly changing dimensions (optional)

Leverages incremental models to optimize future runs

Automation with GitHub Actions:

Runs the entire pipeline daily via CI/CD workflow

Environment variables (Spotify creds, dbt profile) are managed as secrets

dbt build ensures fresh and valid models daily

